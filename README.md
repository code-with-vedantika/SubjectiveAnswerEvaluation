# SubjectiveAnswerEvaluation
Developed a Streamlit-based web application that automatically evaluates studentsâ€™ subjective answers by comparing them with answer keys using Semantic Similarity.


## ğŸ“Œ Project Overview
The Subjective Answer Evaluation System is an AI-powered solution designed to automate the evaluation of descriptive answers in educational assessments. It helps teachers and students by analyzing answers based on **relevance, grammar, content quality, and conceptual understanding**.

---

## ğŸ¯ Objectives
âœ” Automate grading of long subjective answers  
âœ” Improve accuracy and consistency in evaluation  
âœ” Save time for teachers and allow instant feedback  
âœ” Provide self-assessment tools for students  

---

## ğŸš€ Features
ğŸ”¹ Upload student answers  
ğŸ”¹ Compare answers using AI & NLP  
ğŸ”¹ Cosine Similarity and BERT-based Semantic Matching  
ğŸ”¹ Instant Score & Feedback  
ğŸ”¹ Flask web interface for result display  

---

## ğŸ› ï¸ Technologies Used
| Category | Tools |
|----------|-------|
| Programming | Python |
| Web Framework | Flask |
| NLP | BERT, Hugging Face, Cosine Similarity, Jaccard |
| ML Tools | TensorFlow, Keras |
| Frontend | HTML, CSS, JavaScript |
| Deployment | VS Code / Google Colab |

---

## ğŸ§  How It Works (Workflow)
1ï¸âƒ£ Input student answer  
2ï¸âƒ£ Preprocess & tokenize the text  
3ï¸âƒ£ Generate embeddings using BERT  
4ï¸âƒ£ Compare with model answer using similarity metrics  
5ï¸âƒ£ Generate score and detailed feedback  

---

## ğŸ“· Screenshots (Optional)
(Add images like UI, output, architecture)

---

## ğŸ“‚ Folder Structure

